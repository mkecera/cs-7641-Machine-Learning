# Supervised Learning

#### Classification

Some input `x` that maps to a label (e.g., a value belonging to a discrete set). For example `x` could be an image (e.g. photos of faces) and determining the gender of the person.


#### Regression

Some input `x` could map to a real value (e.g., a value that is part of a continual parameter) predicting the age of a person, based off of a photo of their face).

## Terminology

Multiple important terminologies exist that will be used throughout the class:

- ### Instances (Input)
Vectors of attributes that define the input space (e.g. pixel values of a picture, credit score examples containing income, )

- ### Concept 
A function that maps inputs to outputs (e.g., a picture that maps to true or false). References to the more general notion of a concept by describing what something is, or is not (e.g., a set of things that are ). A mapping between objects in the world and membership in a set.

- ### Target Concept
The actual answer we are searching for in the space of multiple concepts, also known as the *hypothesis class*. 

- ### Hypothesis (Class)
The set of all concepts you're trying to entertain. Could be all possible functions (however, given finite data, an infinite space to search).

- ### Sample (Training Set)
Examples of input output pairs (e.g., person profile and credit worthiness)

- Candidate
A *concept* that is believed to be the *target concept*.

- ### Testing Set
Structure is identical to the *training set*, however, **never** overlaps with examples belonging to the *training set*.

---

# Decision Trees


Example: Going on date with someone. 

Input: a list of restaurants with categorical labels (features):

- Type: Italian, French, Thai
- Atmosphere: Fancy, HIW, Casual
- Busy, 
- Status: High End
- Cost: could be discrete or an average number
- Weather (no)
- Date Situation: Hot, (or Not)

Output: True or False

## Presentation vs Algorithms
Decision trees are logical structures containing:

- **Nodes** represent attributes (hungry or not?)
- **Edges** represent values (yes, True or no, False)
- **Leaves** are end states, representing the output

Decision trees can be used to represent logic conditions such as OR or XOR.

- OR, (easy, any) when mapped out in a decision tree, demonstrates *linear* complexity in number of nodes (attributes). For $n$ number of nodes, there will be $n$ operations.

- XOR, (hard, parity) when mapped out in a decision tree, demonstrates *exponential* complexity. For $n$ nodes, there is a bound of $2^n -1$, $O(2^n)$.

### Parity

Even or odd parity define a boolean operation on a set of boolean data.

ODD: if number of attributes that are true is an odd number, the that the output is true, otherwise it's false.

EVEN: if number of attributes	

*Note*: In general in ML, we hope to encounter problems that are more like *any* vs *parity*.

## Expressiveness of Decision Trees

Decision Trees are extremeley expressive, allowing for an extremeley large hypothosis space. We must have clever ways to focus search using algorithims.
The numbe of rows in the truth table of DT is $2^n$.
The number of ways to fill in the outputs is $2^(2^n)$

## ID3 Algorithm

A top-down decision tree learning algorithm:

### Psuedocode

Loop:

 - $A$ $\leftarrow$ best attribute
 - Assign $A$ as decision attribute for *node*
 - For each value of $A$ create a descendent of *node*.
 - Sort training examples to *leaves*.
 - IF Examples perfectly classified $\rightarrow$ **STOP**
 - ELSE Iterate over leaves.
 
### Information Gain (e.g., finding the best attribute)
 
The amount of information gained by picking a particular attribute in Decision Tree making.
Mathmatically, information gain $IG$ quantifies the reduction in randomness over the labels you have with a set of data, based upon knowing the value of a particular attribute. It is defined as:
$$\textrm{IG}(S, A) = \textrm{H}(S) - \sum_v \frac{\left|S_v\right|}{\left|S\right|} \cdot \textrm{H}(S) $$ where: 

- $S$ is the set of training examples and
- $A$ is an attribute.
- $H$ is the entropy, defined as

$$  H(S) = - \sum_v p(v)\log(v) $$

## ID3 Bias 

*Restriction bias* describes the hypothosis set space $H$ we will consider for the learning effort (e.g., decision trees and not quatradic equations).

*Preference bias* desribes the subset of hypothoses, $n$, from the hypothosis set space that we *prefer* ($n \in H$), which is really at the heart of inductive bias.
 
### ID3's biases:
 
1. Good Splits at the top of the tree
2. Correct of Incorrect
3. Prefers shorter trees vs. longer (due to bias #1).

## Other Considerations

### Continuous Attributes (e.g., age, weight, distance)

With continuous attributes, it can make sense to repeat asking about an attribute along a path in a decision tree. For instances, consider age as an attribute, and the desire to continually refine the range an age belongs to by asking T/F questions about

### When do we stop?

- When everything is classified correctly?
- When we've run out of attributes?
- cross validation to prevent overfitting (a big complicated tree, in the case desicion trees).
- Breath-first search

- **Pruning**, Output: Vote

### Regression

- Splitting? Varience
- Output: Average, local linear fit

 
